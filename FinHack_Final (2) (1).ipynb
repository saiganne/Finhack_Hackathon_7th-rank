{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of Fraudlent ATM Transaction\n",
    "\n",
    "PredCatch Analytics' Australian banking client's profitability and reputation are being hit by fraudulent ATM Transactions. So we need to build predictive model to catch such fraudulent transaction in real time and decline them. Let's walk through one of the approach for Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing of the most libraries that are required for model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic Libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split,KFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import datetime\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#Classification Libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Other Data Processing libraries\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import KFold, StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading given data in CSV files into data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading all the given data into respective data frames\n",
    "geo_df = pd.read_csv(r\"D:\\Hackathon\\Data\\Data\\Geo_scores.csv\")\n",
    "instance_df = pd.read_csv(r\"D:\\Hackathon\\Data\\Data\\instance_scores.csv\")\n",
    "lambda_df = pd.read_csv(r\"D:\\Hackathon\\Data\\Data\\Lambda_wts.csv\")\n",
    "qset_df = pd.read_csv(r\"D:\\Hackathon\\Data\\Data\\Qset_tats.csv\")\n",
    "train_df = pd.read_csv(r\"D:\\Hackathon\\Data\\Data\\train.csv\")\n",
    "test_df = pd.read_csv(r\"D:\\Hackathon\\Data\\Data\\test_share.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are multiple values for geo_scores, instance_scores and qset_score for single ID, whereas for lambda_wt are having single value for each group.\n",
    "\n",
    "### Since there are multiple values for three scores, we will group them by ID and aggregrate on mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "284807"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(instance_df['id'].value_counts() > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(lambda_df['Group'].value_counts() > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "284807"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((qset_df['id'].value_counts() > 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "284807"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(geo_df['id'].value_counts() > 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below we're grouping based on ID and aggreating on mean. Further reset index to use ID column while joining with train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grouping by unique columns and aggregating based on mean for exporting the column to train and test data frame\n",
    "geo_scores = geo_df.groupby(['id']).mean().reset_index()\n",
    "instance_score = instance_df.groupby(['id']).mean().reset_index()\n",
    "qset_score = qset_df.groupby(['id']).mean().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging the different scores and lambda wt with train and data. Using left join to retain the original position of records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging the above scores with train data frame using left join to retaing the observation position as it is.\n",
    "train_df_all = pd.merge(train_df, geo_scores, on='id',how='left')\n",
    "train_df_all = pd.merge(train_df_all, instance_score, on='id',how='left')\n",
    "train_df_all = pd.merge(train_df_all,lambda_df, on='Group',how='left')\n",
    "train_df_all = pd.merge(train_df_all, qset_score, on='id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging the above scores with test data frame using left join to retaing the observation position as it is.\n",
    "test_df_all = pd.merge(test_df, geo_scores, on='id',how='left')\n",
    "test_df_all = pd.merge(test_df_all, instance_score, on='id',how='left')\n",
    "test_df_all = pd.merge(test_df_all, lambda_df, on='Group',how='left')\n",
    "test_df_all = pd.merge(test_df_all, qset_score, on='id',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's check if there is any data processing required for modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(227845, 32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking for number of features and observations in train data\n",
    "train_df_all.shape\n",
    "\n",
    "#There are 32 features with 227845 observations(This includes Target variable as well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56962, 31)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking for number of features and observations in test data\n",
    "test_df_all.shape\n",
    "\n",
    "#There are 3 features with 227845 observations(This doesn't include Target variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are there any NA values in our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#looking at number of NA's\n",
    "train_df_all.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#looking at number of NA's\n",
    "test_df_all.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could see there are no values in both train and test data, hence no imputations are required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                        int64\n",
       "Group                    object\n",
       "Per1                    float64\n",
       "Per2                    float64\n",
       "Per3                    float64\n",
       "Per4                    float64\n",
       "Per5                    float64\n",
       "Per6                    float64\n",
       "Per7                    float64\n",
       "Per8                    float64\n",
       "Per9                    float64\n",
       "Dem1                    float64\n",
       "Dem2                    float64\n",
       "Dem3                    float64\n",
       "Dem4                    float64\n",
       "Dem5                    float64\n",
       "Dem6                    float64\n",
       "Dem7                    float64\n",
       "Dem8                    float64\n",
       "Dem9                    float64\n",
       "Cred1                   float64\n",
       "Cred2                   float64\n",
       "Cred3                   float64\n",
       "Cred4                   float64\n",
       "Cred5                   float64\n",
       "Cred6                   float64\n",
       "Normalised_FNT          float64\n",
       "Target                    int64\n",
       "geo_score               float64\n",
       "instance_scores         float64\n",
       "lambda_wt               float64\n",
       "qsets_normalized_tat    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Verifying with data types in our data\n",
    "train_df_all.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All data types are numeric(either integer or float64) except Group, let's see if dummies are required for Group variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1301"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking for number of unique value of column group\n",
    "len(train_df['Group'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could see there are around 1301 unique values in our group column, hence further breakdown and see how many groups are having observations more than 150."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "316"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(train_df['Group'].value_counts()>150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 316 groups with 150 observation against each group, which signifies that it's kind of ID and also we had Lambda_wt corresponding to each Group. So group can be ignored in our modelling process.\n",
    "\n",
    "Similarly ID value is unique for each of the observation, hence ignoring ID. Below dropping ID and Group columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing ID since they don't make any sense in our prediction and presently Group not considered as it will involve huge number of dummies\n",
    "train = train_df_all.drop(['id','Group'],axis=1)\n",
    "test = test_df_all.drop(['id','Group'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our data now has no missing values and all data types  are relevant for modelling. Now checking for distribution of our target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Frauds (Target:0) 99.83 % of the dataset\n",
      "Frauds (Target:1) 0.17 % of the dataset\n"
     ]
    }
   ],
   "source": [
    "#checking the class distribution\n",
    "print('No Frauds (Target:0)', round(train_df['Target'].value_counts()[0]/len(train) * 100,2), '% of the dataset')\n",
    "print('Frauds (Target:1)', round(train_df['Target'].value_counts()[1]/len(train) * 100,2), '% of the dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clearly 99.83% are no fraud and 0.17 fraud, our data is highly imbalanced. We need to take care of this during modelling. Else our predictions will be more towards No Fraud(Class 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Per1</th>\n",
       "      <th>Per2</th>\n",
       "      <th>Per3</th>\n",
       "      <th>Per4</th>\n",
       "      <th>Per5</th>\n",
       "      <th>Per6</th>\n",
       "      <th>Per7</th>\n",
       "      <th>Per8</th>\n",
       "      <th>Per9</th>\n",
       "      <th>Dem1</th>\n",
       "      <th>...</th>\n",
       "      <th>Cred3</th>\n",
       "      <th>Cred4</th>\n",
       "      <th>Cred5</th>\n",
       "      <th>Cred6</th>\n",
       "      <th>Normalised_FNT</th>\n",
       "      <th>Target</th>\n",
       "      <th>geo_score</th>\n",
       "      <th>instance_scores</th>\n",
       "      <th>lambda_wt</th>\n",
       "      <th>qsets_normalized_tat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>227845.000000</td>\n",
       "      <td>227845.000000</td>\n",
       "      <td>227845.000000</td>\n",
       "      <td>227845.000000</td>\n",
       "      <td>227845.000000</td>\n",
       "      <td>227845.000000</td>\n",
       "      <td>227845.000000</td>\n",
       "      <td>227845.000000</td>\n",
       "      <td>227845.000000</td>\n",
       "      <td>227845.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>227845.000000</td>\n",
       "      <td>227845.000000</td>\n",
       "      <td>227845.000000</td>\n",
       "      <td>227845.000000</td>\n",
       "      <td>227845.000000</td>\n",
       "      <td>227845.000000</td>\n",
       "      <td>227845.000000</td>\n",
       "      <td>227845.000000</td>\n",
       "      <td>227845.000000</td>\n",
       "      <td>227845.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.666006</td>\n",
       "      <td>0.667701</td>\n",
       "      <td>0.666315</td>\n",
       "      <td>0.666687</td>\n",
       "      <td>0.666723</td>\n",
       "      <td>0.667378</td>\n",
       "      <td>0.666934</td>\n",
       "      <td>0.666279</td>\n",
       "      <td>0.666688</td>\n",
       "      <td>0.666576</td>\n",
       "      <td>...</td>\n",
       "      <td>0.666755</td>\n",
       "      <td>0.666878</td>\n",
       "      <td>0.666566</td>\n",
       "      <td>0.666776</td>\n",
       "      <td>-227.954170</td>\n",
       "      <td>0.001729</td>\n",
       "      <td>-0.000478</td>\n",
       "      <td>-0.000123</td>\n",
       "      <td>0.000350</td>\n",
       "      <td>0.000115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.654133</td>\n",
       "      <td>0.548305</td>\n",
       "      <td>0.506357</td>\n",
       "      <td>0.471956</td>\n",
       "      <td>0.461393</td>\n",
       "      <td>0.444573</td>\n",
       "      <td>0.415657</td>\n",
       "      <td>0.401546</td>\n",
       "      <td>0.366537</td>\n",
       "      <td>0.340436</td>\n",
       "      <td>...</td>\n",
       "      <td>0.174204</td>\n",
       "      <td>0.160803</td>\n",
       "      <td>0.135762</td>\n",
       "      <td>0.111612</td>\n",
       "      <td>61.951661</td>\n",
       "      <td>0.041548</td>\n",
       "      <td>1.076016</td>\n",
       "      <td>1.091488</td>\n",
       "      <td>0.957957</td>\n",
       "      <td>0.945602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-18.136667</td>\n",
       "      <td>-23.573333</td>\n",
       "      <td>-15.443333</td>\n",
       "      <td>-1.226667</td>\n",
       "      <td>-37.246667</td>\n",
       "      <td>-8.053333</td>\n",
       "      <td>-13.853333</td>\n",
       "      <td>-23.740000</td>\n",
       "      <td>-3.810000</td>\n",
       "      <td>-0.893333</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.766667</td>\n",
       "      <td>-0.080000</td>\n",
       "      <td>-6.856667</td>\n",
       "      <td>-4.476667</td>\n",
       "      <td>-250.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-25.983333</td>\n",
       "      <td>-24.590000</td>\n",
       "      <td>-19.210000</td>\n",
       "      <td>-31.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.360000</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>0.370000</td>\n",
       "      <td>0.383333</td>\n",
       "      <td>0.436667</td>\n",
       "      <td>0.410000</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>0.596667</td>\n",
       "      <td>0.453333</td>\n",
       "      <td>0.413333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.556667</td>\n",
       "      <td>0.643333</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>-248.617500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.430000</td>\n",
       "      <td>-0.540000</td>\n",
       "      <td>-0.430000</td>\n",
       "      <td>-0.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.670000</td>\n",
       "      <td>0.690000</td>\n",
       "      <td>0.726667</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.576667</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.673333</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.656667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.673333</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.670000</td>\n",
       "      <td>-244.510000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>-0.090000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>-0.070000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.103333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>1.010000</td>\n",
       "      <td>0.913333</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.856667</td>\n",
       "      <td>0.776667</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.913333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.746667</td>\n",
       "      <td>0.696667</td>\n",
       "      <td>0.693333</td>\n",
       "      <td>-230.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.483333</td>\n",
       "      <td>8.020000</td>\n",
       "      <td>3.793333</td>\n",
       "      <td>6.163333</td>\n",
       "      <td>12.266667</td>\n",
       "      <td>25.100000</td>\n",
       "      <td>40.863333</td>\n",
       "      <td>7.336667</td>\n",
       "      <td>5.863333</td>\n",
       "      <td>4.673333</td>\n",
       "      <td>...</td>\n",
       "      <td>3.173333</td>\n",
       "      <td>1.840000</td>\n",
       "      <td>11.203333</td>\n",
       "      <td>11.950000</td>\n",
       "      <td>6172.790000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.850000</td>\n",
       "      <td>23.750000</td>\n",
       "      <td>10.530000</td>\n",
       "      <td>10.233333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Per1           Per2           Per3           Per4  \\\n",
       "count  227845.000000  227845.000000  227845.000000  227845.000000   \n",
       "mean        0.666006       0.667701       0.666315       0.666687   \n",
       "std         0.654133       0.548305       0.506357       0.471956   \n",
       "min       -18.136667     -23.573333     -15.443333      -1.226667   \n",
       "25%         0.360000       0.470000       0.370000       0.383333   \n",
       "50%         0.670000       0.690000       0.726667       0.660000   \n",
       "75%         1.103333       0.933333       1.010000       0.913333   \n",
       "max         1.483333       8.020000       3.793333       6.163333   \n",
       "\n",
       "                Per5           Per6           Per7           Per8  \\\n",
       "count  227845.000000  227845.000000  227845.000000  227845.000000   \n",
       "mean        0.666723       0.667378       0.666934       0.666279   \n",
       "std         0.461393       0.444573       0.415657       0.401546   \n",
       "min       -37.246667      -8.053333     -13.853333     -23.740000   \n",
       "25%         0.436667       0.410000       0.483333       0.596667   \n",
       "50%         0.650000       0.576667       0.680000       0.673333   \n",
       "75%         0.870000       0.800000       0.856667       0.776667   \n",
       "max        12.266667      25.100000      40.863333       7.336667   \n",
       "\n",
       "                Per9           Dem1          ...                   Cred3  \\\n",
       "count  227845.000000  227845.000000          ...           227845.000000   \n",
       "mean        0.666688       0.666576          ...                0.666755   \n",
       "std         0.366537       0.340436          ...                0.174204   \n",
       "min        -3.810000      -0.893333          ...               -2.766667   \n",
       "25%         0.453333       0.413333          ...                0.560000   \n",
       "50%         0.650000       0.656667          ...                0.673333   \n",
       "75%         0.866667       0.913333          ...                0.783333   \n",
       "max         5.863333       4.673333          ...                3.173333   \n",
       "\n",
       "               Cred4          Cred5          Cred6  Normalised_FNT  \\\n",
       "count  227845.000000  227845.000000  227845.000000   227845.000000   \n",
       "mean        0.666878       0.666566       0.666776     -227.954170   \n",
       "std         0.160803       0.135762       0.111612       61.951661   \n",
       "min        -0.080000      -6.856667      -4.476667     -250.000000   \n",
       "25%         0.556667       0.643333       0.650000     -248.617500   \n",
       "50%         0.650000       0.666667       0.670000     -244.510000   \n",
       "75%         0.746667       0.696667       0.693333     -230.750000   \n",
       "max         1.840000      11.203333      11.950000     6172.790000   \n",
       "\n",
       "              Target      geo_score  instance_scores      lambda_wt  \\\n",
       "count  227845.000000  227845.000000    227845.000000  227845.000000   \n",
       "mean        0.001729      -0.000478        -0.000123       0.000350   \n",
       "std         0.041548       1.076016         1.091488       0.957957   \n",
       "min         0.000000     -25.983333       -24.590000     -19.210000   \n",
       "25%         0.000000      -0.430000        -0.540000      -0.430000   \n",
       "50%         0.000000       0.150000        -0.090000       0.050000   \n",
       "75%         0.000000       0.650000         0.450000       0.490000   \n",
       "max         1.000000       7.850000        23.750000      10.530000   \n",
       "\n",
       "       qsets_normalized_tat  \n",
       "count         227845.000000  \n",
       "mean               0.000115  \n",
       "std                0.945602  \n",
       "min              -31.450000  \n",
       "25%               -0.520000  \n",
       "50%               -0.070000  \n",
       "75%                0.437500  \n",
       "max               10.233333  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#looking how features spread in data\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All features are spread in almost same scale, only Normalised_FNT is between [-250,6172.79].. Let's scale this so that this variable is not given more weightage in our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could see that Normalized_FNT has a different scale when compared to other features, hence scaling for modelling purposes\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "# RobustScaler is less prone to outliers. \n",
    "#Robust scaler is similar to MIN-MAX scaler, however its uses interquartile range which makes it less prone to outliers\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "rob_scaler = RobustScaler()\n",
    "\n",
    "train['Normalised_FNT_Scaled'] = rob_scaler.fit_transform(train['Normalised_FNT'].values.reshape(-1,1))\n",
    "test['Normalised_FNT_Scaled'] = rob_scaler.fit_transform(test['Normalised_FNT'].values.reshape(-1,1))\n",
    "\n",
    "#Now dropping Normalised_FNT, as we have added normalised FNT with scale\n",
    "train.drop([\"Normalised_FNT\"], axis=1, inplace=True)\n",
    "test.drop([\"Normalised_FNT\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Per1</th>\n",
       "      <th>Per2</th>\n",
       "      <th>Per3</th>\n",
       "      <th>Per4</th>\n",
       "      <th>Per5</th>\n",
       "      <th>Per6</th>\n",
       "      <th>Per7</th>\n",
       "      <th>Per8</th>\n",
       "      <th>Per9</th>\n",
       "      <th>Dem1</th>\n",
       "      <th>...</th>\n",
       "      <th>Cred3</th>\n",
       "      <th>Cred4</th>\n",
       "      <th>Cred5</th>\n",
       "      <th>Cred6</th>\n",
       "      <th>Target</th>\n",
       "      <th>geo_score</th>\n",
       "      <th>instance_scores</th>\n",
       "      <th>lambda_wt</th>\n",
       "      <th>qsets_normalized_tat</th>\n",
       "      <th>Normalised_FNT_Scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>227845.000000</td>\n",
       "      <td>227845.000000</td>\n",
       "      <td>227845.000000</td>\n",
       "      <td>227845.000000</td>\n",
       "      <td>227845.000000</td>\n",
       "      <td>227845.000000</td>\n",
       "      <td>227845.000000</td>\n",
       "      <td>227845.000000</td>\n",
       "      <td>227845.000000</td>\n",
       "      <td>227845.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>227845.000000</td>\n",
       "      <td>227845.000000</td>\n",
       "      <td>227845.000000</td>\n",
       "      <td>227845.000000</td>\n",
       "      <td>227845.000000</td>\n",
       "      <td>227845.000000</td>\n",
       "      <td>227845.000000</td>\n",
       "      <td>227845.000000</td>\n",
       "      <td>227845.000000</td>\n",
       "      <td>227845.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.666006</td>\n",
       "      <td>0.667701</td>\n",
       "      <td>0.666315</td>\n",
       "      <td>0.666687</td>\n",
       "      <td>0.666723</td>\n",
       "      <td>0.667378</td>\n",
       "      <td>0.666934</td>\n",
       "      <td>0.666279</td>\n",
       "      <td>0.666688</td>\n",
       "      <td>0.666576</td>\n",
       "      <td>...</td>\n",
       "      <td>0.666755</td>\n",
       "      <td>0.666878</td>\n",
       "      <td>0.666566</td>\n",
       "      <td>0.666776</td>\n",
       "      <td>0.001729</td>\n",
       "      <td>-0.000478</td>\n",
       "      <td>-0.000123</td>\n",
       "      <td>0.000350</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>0.926589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.654133</td>\n",
       "      <td>0.548305</td>\n",
       "      <td>0.506357</td>\n",
       "      <td>0.471956</td>\n",
       "      <td>0.461393</td>\n",
       "      <td>0.444573</td>\n",
       "      <td>0.415657</td>\n",
       "      <td>0.401546</td>\n",
       "      <td>0.366537</td>\n",
       "      <td>0.340436</td>\n",
       "      <td>...</td>\n",
       "      <td>0.174204</td>\n",
       "      <td>0.160803</td>\n",
       "      <td>0.135762</td>\n",
       "      <td>0.111612</td>\n",
       "      <td>0.041548</td>\n",
       "      <td>1.076016</td>\n",
       "      <td>1.091488</td>\n",
       "      <td>0.957957</td>\n",
       "      <td>0.945602</td>\n",
       "      <td>3.467282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-18.136667</td>\n",
       "      <td>-23.573333</td>\n",
       "      <td>-15.443333</td>\n",
       "      <td>-1.226667</td>\n",
       "      <td>-37.246667</td>\n",
       "      <td>-8.053333</td>\n",
       "      <td>-13.853333</td>\n",
       "      <td>-23.740000</td>\n",
       "      <td>-3.810000</td>\n",
       "      <td>-0.893333</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.766667</td>\n",
       "      <td>-0.080000</td>\n",
       "      <td>-6.856667</td>\n",
       "      <td>-4.476667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-25.983333</td>\n",
       "      <td>-24.590000</td>\n",
       "      <td>-19.210000</td>\n",
       "      <td>-31.450000</td>\n",
       "      <td>-0.307262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.360000</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>0.370000</td>\n",
       "      <td>0.383333</td>\n",
       "      <td>0.436667</td>\n",
       "      <td>0.410000</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>0.596667</td>\n",
       "      <td>0.453333</td>\n",
       "      <td>0.413333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.556667</td>\n",
       "      <td>0.643333</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.430000</td>\n",
       "      <td>-0.540000</td>\n",
       "      <td>-0.430000</td>\n",
       "      <td>-0.520000</td>\n",
       "      <td>-0.229887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.670000</td>\n",
       "      <td>0.690000</td>\n",
       "      <td>0.726667</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.576667</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.673333</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.656667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.673333</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.670000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>-0.090000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>-0.070000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.103333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>1.010000</td>\n",
       "      <td>0.913333</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.856667</td>\n",
       "      <td>0.776667</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.913333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.746667</td>\n",
       "      <td>0.696667</td>\n",
       "      <td>0.693333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.770113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.483333</td>\n",
       "      <td>8.020000</td>\n",
       "      <td>3.793333</td>\n",
       "      <td>6.163333</td>\n",
       "      <td>12.266667</td>\n",
       "      <td>25.100000</td>\n",
       "      <td>40.863333</td>\n",
       "      <td>7.336667</td>\n",
       "      <td>5.863333</td>\n",
       "      <td>4.673333</td>\n",
       "      <td>...</td>\n",
       "      <td>3.173333</td>\n",
       "      <td>1.840000</td>\n",
       "      <td>11.203333</td>\n",
       "      <td>11.950000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.850000</td>\n",
       "      <td>23.750000</td>\n",
       "      <td>10.530000</td>\n",
       "      <td>10.233333</td>\n",
       "      <td>359.160487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Per1           Per2           Per3           Per4  \\\n",
       "count  227845.000000  227845.000000  227845.000000  227845.000000   \n",
       "mean        0.666006       0.667701       0.666315       0.666687   \n",
       "std         0.654133       0.548305       0.506357       0.471956   \n",
       "min       -18.136667     -23.573333     -15.443333      -1.226667   \n",
       "25%         0.360000       0.470000       0.370000       0.383333   \n",
       "50%         0.670000       0.690000       0.726667       0.660000   \n",
       "75%         1.103333       0.933333       1.010000       0.913333   \n",
       "max         1.483333       8.020000       3.793333       6.163333   \n",
       "\n",
       "                Per5           Per6           Per7           Per8  \\\n",
       "count  227845.000000  227845.000000  227845.000000  227845.000000   \n",
       "mean        0.666723       0.667378       0.666934       0.666279   \n",
       "std         0.461393       0.444573       0.415657       0.401546   \n",
       "min       -37.246667      -8.053333     -13.853333     -23.740000   \n",
       "25%         0.436667       0.410000       0.483333       0.596667   \n",
       "50%         0.650000       0.576667       0.680000       0.673333   \n",
       "75%         0.870000       0.800000       0.856667       0.776667   \n",
       "max        12.266667      25.100000      40.863333       7.336667   \n",
       "\n",
       "                Per9           Dem1          ...                    Cred3  \\\n",
       "count  227845.000000  227845.000000          ...            227845.000000   \n",
       "mean        0.666688       0.666576          ...                 0.666755   \n",
       "std         0.366537       0.340436          ...                 0.174204   \n",
       "min        -3.810000      -0.893333          ...                -2.766667   \n",
       "25%         0.453333       0.413333          ...                 0.560000   \n",
       "50%         0.650000       0.656667          ...                 0.673333   \n",
       "75%         0.866667       0.913333          ...                 0.783333   \n",
       "max         5.863333       4.673333          ...                 3.173333   \n",
       "\n",
       "               Cred4          Cred5          Cred6         Target  \\\n",
       "count  227845.000000  227845.000000  227845.000000  227845.000000   \n",
       "mean        0.666878       0.666566       0.666776       0.001729   \n",
       "std         0.160803       0.135762       0.111612       0.041548   \n",
       "min        -0.080000      -6.856667      -4.476667       0.000000   \n",
       "25%         0.556667       0.643333       0.650000       0.000000   \n",
       "50%         0.650000       0.666667       0.670000       0.000000   \n",
       "75%         0.746667       0.696667       0.693333       0.000000   \n",
       "max         1.840000      11.203333      11.950000       1.000000   \n",
       "\n",
       "           geo_score  instance_scores      lambda_wt  qsets_normalized_tat  \\\n",
       "count  227845.000000    227845.000000  227845.000000         227845.000000   \n",
       "mean       -0.000478        -0.000123       0.000350              0.000115   \n",
       "std         1.076016         1.091488       0.957957              0.945602   \n",
       "min       -25.983333       -24.590000     -19.210000            -31.450000   \n",
       "25%        -0.430000        -0.540000      -0.430000             -0.520000   \n",
       "50%         0.150000        -0.090000       0.050000             -0.070000   \n",
       "75%         0.650000         0.450000       0.490000              0.437500   \n",
       "max         7.850000        23.750000      10.530000             10.233333   \n",
       "\n",
       "       Normalised_FNT_Scaled  \n",
       "count          227845.000000  \n",
       "mean                0.926589  \n",
       "std                 3.467282  \n",
       "min                -0.307262  \n",
       "25%                -0.229887  \n",
       "50%                 0.000000  \n",
       "75%                 0.770113  \n",
       "max               359.160487  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now all features are on similar scale, we can proceed further with modelling. Separating Features and Target variable for Fitting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop('Target', axis=1)\n",
    "Y = train['Target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting data into train and test for validating model built. Using Stratified sampling so that splitting is made by preserving the percentage of samples for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [225122  61988 197455 ... 195669 152118 100486] Test: [197832 111272  39995 ... 161441 162197  66521]\n",
      "Train: [ 94872 216389  49742 ... 212564  15634  28205] Test: [218589 111697 194742 ...   2918 166454  55967]\n",
      "Train: [ 32315 180277 157046 ... 137763 192051 182988] Test: [ 74377  81640 166170 ... 119451  81607 204468]\n",
      "Train: [ 43131 115016 219145 ... 219246 193635 214208] Test: [112229 227312  70281 ...  18812  45834 188756]\n",
      "Train: [169045  18030 143025 ...  18168  91155 146404] Test: [223113 114665 210205 ...  40527 224978 204904]\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Label Distributions: \n",
      "\n",
      "[0.99827185 0.00172815]\n",
      "[0.99826637 0.00173363]\n"
     ]
    }
   ],
   "source": [
    "#Our data has great imbalance, \n",
    "#i.e, there are very less records for fraud class(Target:1). hence using this for sampling from each group\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "\n",
    "for train_index, test_index in sss.split(X, Y):\n",
    "    print(\"Train:\", train_index, \"Test:\", test_index)\n",
    "    actual_Xtrain, actual_Xtest = X.iloc[train_index], X.iloc[test_index]\n",
    "    actual_Ytrain, actual_Ytest = Y.iloc[train_index], Y.iloc[test_index]\n",
    "\n",
    "actual_Xtrain = actual_Xtrain.values\n",
    "actual_Xtest = actual_Xtest.values\n",
    "actual_Ytrain = actual_Ytrain.values\n",
    "actual_Ytest = actual_Ytest.values\n",
    "\n",
    "\n",
    "train_unique_label, train_counts_label = np.unique(actual_Ytrain, return_counts=True)\n",
    "test_unique_label, test_counts_label = np.unique(actual_Ytest, return_counts=True)\n",
    "print('---' * 85)\n",
    "\n",
    "print('Label Distributions: \\n')\n",
    "print(train_counts_label/ len(actual_Ytrain))\n",
    "print(test_counts_label/ len(actual_Ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### With above splitting data into train & test using Stratified sampling, retaining only the last split. Modelling can be done on each split and check how the performance varies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "The basic model for any classification problem is Logistic regression. Since our data is imbalance use class_weight=\"balance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Logistic Regression Model took 120.6010115146637 sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t0 = time.time()\n",
    "logr=LogisticRegression(penalty=\"l1\",class_weight=\"balanced\",random_state=2)\n",
    "logr.fit(actual_Xtrain, actual_Ytrain)\n",
    "t1 = time.time()\n",
    "print(\"Fitting Logistic Regression Model took {} sec\".format(t1 - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logr is model built, now let's make predictions on the validation data.\n",
    "\n",
    "Predicting hard classes for imbalanced data with KS Score will tend to favour majority class. Hence using logr.predict which predict class labels for test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting on our validaton data\n",
    "pred_test = logr.predict(actual_Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5260035076314027"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking for AUC Score metric\n",
    "roc_auc_score(pred_test,actual_Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0957475309656366"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking for Cohen Kappa Score metric\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "cohen_kappa_score(pred_test,actual_Ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above metrics are obtained from a basic model.. We can further train model on entire training data and make predictions on test share data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting on entire train data\n",
    "import time\n",
    "t0 = time.time()\n",
    "logr=LogisticRegression(penalty=\"l1\",class_weight=\"balanced\",random_state=2)\n",
    "logr.fit(X, Y)\n",
    "t1 = time.time()\n",
    "print(\"Fitting Logistic Regression Model took {} sec\".format(t1 - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction on given test data\n",
    "pred_test_share = logr.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting into data frame with corresponding ID for submission\n",
    "predictions = pd.DataFrame(list(zip(test_df_all['id'],pred_test_share)),columns=[\"id\",\"Target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomized search with XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's train model using XGBoost, and check the model performs\n",
    "\n",
    "Due to time and system performance constraints, considered only to tune below 4 parameters. It'll be good option to try building model by tuning other parameters as well, there are chances of getting an improved performance\n",
    "\n",
    "XGB is widely used due to it's ability to penalise errors which is a disadvantage for GBM. Also XGB can handle missing values on it's own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters for training XGBoost, limited to 3 parameters due system processing and time constraints\n",
    "param_dist_xgb = {\n",
    "              \"max_depth\": [2,3,4,5,6,7],\n",
    "              \"learning_rate\":[0.01,0.05,0.1,0.2,0.3,0.5],\n",
    "    #\"min_child_weight\":[4,5,6],\n",
    "              #\"subsample\":[i/10.0 for i in range(6,10)],\n",
    " #\"colsample_bytree\":[i/10.0 for i in range(6,10)],\n",
    "               #\"reg_alpha\":[1e-5, 1e-2, 0.1, 1, 100],\n",
    "              #\"gamma\":[i/10.0 for i in range(0,5)],\n",
    "    \"n_estimators\":[100,200,500,700],\n",
    "    \"scale_pos_weight\":[2,3,4,5,6,7,8,9]\n",
    "                  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Again limiting to 10 iterations, (With above number of parameters number of combinations will be over 1000)..\n",
    "#10% of possible combinations will give us an more probability of getting best params\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "n_iter=10\n",
    "clf=XGBClassifier(objective='binary:logistic')\n",
    "random_search=RandomizedSearchCV(clf,n_jobs=-1,verbose=2,cv=10,n_iter=n_iter,scoring='roc_auc',\n",
    "                                 param_distributions=param_dist_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed: 28.8min\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed: 136.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution through Random Search for XGB took 8294.710326910019 sec\n"
     ]
    }
   ],
   "source": [
    "#fitting on validation train data\n",
    "import time\n",
    "t0 = time.time()\n",
    "random_search.fit(actual_Xtrain,actual_Ytrain)\n",
    "t1 = time.time()\n",
    "print(\"Execution through Random Search for XGB took {} sec\".format(t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best fit\n",
    "best_XGB_fit = random_search.best_estimator_\n",
    "#best_params saved from our iteration(learning_rate=0.01,n_estimators=700,max_depth=2,scale_pos_weight=6 )\n",
    "#this keeps varies with different runs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.01,\n",
       " 'max_depth': 2,\n",
       " 'n_estimators': 700,\n",
       " 'scale_pos_weight': 6}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction on validation test data\n",
    "pred_XGB = best_XGB_fit.predict(actual_Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9100915424803666"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(pred_XGB,actual_Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8149678892871739"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohen_kappa_score(pred_XGB,actual_Ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There's lot of improvement in both roc_auc_score and cohen_kappa_score when compared to Logistic regression\n",
    "\n",
    "Let's train model on our entire training data and make predictions on test_share data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.01, max_delta_step=0,\n",
       "       max_depth=2, min_child_weight=1, missing=None, n_estimators=700,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=6, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fitting on entire train data\n",
    "best_XGB_fit.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predictions on given test data\n",
    "pred_test_share=best_XGB_fit.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combinig results into data frame with respective ID's for submission\n",
    "predictions = pd.DataFrame(list(zip(test_df_all['id'],pred_test_share)),columns=[\"id\",\"Target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomized Search with Random Forests\n",
    "\n",
    "### Building model with hyper parameter tuning using Random Forests to see if we get improved performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forests using randomized search wiht below parameters\n",
    "from scipy.stats import randint as sp_randint\n",
    "param_dist_rf = {\"n_estimators\":[10,100,500,700],\n",
    "              \"max_depth\": [3,5, None],\n",
    "              \"max_features\": sp_randint(5, 13),\n",
    "              \"min_samples_split\": sp_randint(5, 11),\n",
    "              \"min_samples_leaf\": sp_randint(5, 11),\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   29.7s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    4.7s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   20.4s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   48.5s\n",
      "[Parallel(n_jobs=-1)]: Done 700 out of 700 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=4)]: Done 700 out of 700 | elapsed:    1.7s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=4)]: Done 700 out of 700 | elapsed:    3.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   21.2s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   47.1s\n",
      "[Parallel(n_jobs=-1)]: Done 700 out of 700 | elapsed:  1.2min finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=4)]: Done 700 out of 700 | elapsed:    1.7s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=4)]: Done 700 out of 700 | elapsed:    3.8s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    5.0s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   20.9s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   47.4s\n",
      "[Parallel(n_jobs=-1)]: Done 700 out of 700 | elapsed:  1.2min finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=4)]: Done 700 out of 700 | elapsed:    1.6s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=4)]: Done 700 out of 700 | elapsed:    3.6s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    6.3s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   14.3s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    6.3s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   14.0s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    6.5s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   14.7s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   21.5s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 700 out of 700 | elapsed:  5.8min finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=4)]: Done 700 out of 700 | elapsed:    2.5s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=4)]: Done 700 out of 700 | elapsed:    5.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   22.2s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 700 out of 700 | elapsed:  5.6min finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=4)]: Done 700 out of 700 | elapsed:    2.4s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=4)]: Done 700 out of 700 | elapsed:    5.8s finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   18.8s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 700 out of 700 | elapsed:  5.1min finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=4)]: Done 700 out of 700 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=4)]: Done 700 out of 700 | elapsed:    5.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    6.3s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   14.2s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    6.7s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   14.4s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    6.3s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   13.9s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    9.9s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   21.2s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    8.6s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   20.7s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    9.1s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   20.6s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    5.3s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    4.7s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    4.2s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    5.7s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   23.8s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   54.2s\n",
      "[Parallel(n_jobs=-1)]: Done 700 out of 700 | elapsed:  1.4min finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=4)]: Done 700 out of 700 | elapsed:    1.6s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=4)]: Done 700 out of 700 | elapsed:    3.6s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    5.0s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   23.3s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   53.1s\n",
      "[Parallel(n_jobs=-1)]: Done 700 out of 700 | elapsed:  1.4min finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=4)]: Done 700 out of 700 | elapsed:    1.6s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=4)]: Done 700 out of 700 | elapsed:    3.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   22.8s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   53.0s\n",
      "[Parallel(n_jobs=-1)]: Done 700 out of 700 | elapsed:  1.4min finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=4)]: Done 700 out of 700 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    2.4s\n",
      "[Parallel(n_jobs=4)]: Done 700 out of 700 | elapsed:    3.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   20.2s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  3.9min finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=4)]: Done 500 out of 500 | elapsed:    1.9s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    3.7s\n",
      "[Parallel(n_jobs=4)]: Done 500 out of 500 | elapsed:    4.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   21.0s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  4.0min finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=4)]: Done 500 out of 500 | elapsed:    1.6s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=4)]: Done 500 out of 500 | elapsed:    3.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   20.3s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  3.8min finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=4)]: Done 500 out of 500 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=4)]: Done 500 out of 500 | elapsed:    3.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   11.9s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   53.5s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 700 out of 700 | elapsed:  3.1min finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=4)]: Done 700 out of 700 | elapsed:    1.7s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=4)]: Done 700 out of 700 | elapsed:    4.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   11.7s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   51.4s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 700 out of 700 | elapsed:  3.4min finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=4)]: Done 700 out of 700 | elapsed:    1.7s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=4)]: Done 700 out of 700 | elapsed:    4.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   11.6s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   51.5s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 700 out of 700 | elapsed:  3.1min finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=4)]: Done 700 out of 700 | elapsed:    1.7s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=4)]: Done 700 out of 700 | elapsed:    4.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    7.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "          estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=1,\n",
       "            warm_start=False),\n",
       "          fit_params=None, iid='warn', n_iter=10, n_jobs=None,\n",
       "          param_distributions={'n_estimators': [10, 100, 500, 700], 'max_depth': [3, 5, None], 'max_features': <scipy.stats._distn_infrastructure.rv_frozen object at 0x0000020186799518>, 'min_samples_split': <scipy.stats._distn_infrastructure.rv_frozen object at 0x0000020186799D68>, 'min_samples_leaf': <scipy.stats._distn_infrastructure.rv_frozen object at 0x00000201864778D0>, 'bootstrap': [True, False], 'criterion': ['gini', 'entropy']},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring=None, verbose=0)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fitting on validation train data\n",
    "n_iter_search = 10\n",
    "rf_clf = RandomForestClassifier(verbose=1,n_jobs=-1)\n",
    "random_search = RandomizedSearchCV(rf_clf, param_distributions=param_dist_rf,\n",
    "                                   n_iter=n_iter_search)\n",
    "random_search.fit(actual_Xtrain, actual_Ytrain)\n",
    "#This takes quite some time to execute and increases with number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': False,\n",
       " 'criterion': 'entropy',\n",
       " 'max_depth': None,\n",
       " 'max_features': 12,\n",
       " 'min_samples_leaf': 6,\n",
       " 'min_samples_split': 8,\n",
       " 'n_estimators': 10}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best estimator\n",
    "rand_best=random_search.best_estimator_\n",
    "#Best Params taken from Iterations run\n",
    "#n_estimators=10,max_depth=None,max_features=12,min_samples_leaf=6,min_samples_split=8,criterion=entropy,class=balanced\n",
    "#bootstrap=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted      0   1\n",
      "real                \n",
      "0          45487   3\n",
      "1             17  62\n"
     ]
    }
   ],
   "source": [
    "#prediction on validation test data and viewing confusion matrix\n",
    "predicted_rf=rand_best.predict(actual_Xtest)\n",
    "\n",
    "df_test=pd.DataFrame(list(zip(actual_Ytest,predicted_rf)),columns=[\"real\",\"predicted\"])\n",
    "\n",
    "k=pd.crosstab(df_test['real'],df_test[\"predicted\"])\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8923720890110778"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking with AUC and cohen kappa metrics\n",
    "roc_auc_score(actual_Ytest,predicted_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8608933971908824"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohen_kappa_score(actual_Ytest,predicted_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With random forests there is little decrease in AUC score whereas there is improvement in Cohen_kappa_score\n",
    "\n",
    "Let's build our model on entire train data and make predictions on test_share data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    9.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, class_weight=None,\n",
       "            criterion='entropy', max_depth=None, max_features=12,\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=6,\n",
       "            min_samples_split=8, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=-1, oob_score=False, random_state=None,\n",
       "            verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fitting on entire train data\n",
    "rand_best.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "#predictions on given test share data\n",
    "pred_test_share=rand_best.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combining into data frame with respective ID's\n",
    "predictions = pd.DataFrame(list(zip(test_df_all['id'],pred_test_share)),columns=[\"id\",\"Target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using SMOTE for Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Directly using parameters from previous Randomized params for faster view on results\n",
    "#saved from our previous iteration without balancing(learning_rate=0.01,n_estimators=700,max_depth=2,scale_pos_weight=6 )\n",
    "clf=XGBClassifier(objective='binary:logistic',max_depth=2, learning_rate=0.01, n_estimators=700,scale_pos_weight=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using SMOTE to balance our validation train data set\n",
    "sm = SMOTE(ratio='minority', random_state=42)\n",
    "# This will be the data were we are going to \n",
    "Xsm_train, Ysm_train = sm.fit_sample(actual_Xtrain, actual_Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for simple XGBoost(for given set of params) is 468.5534701347351\n"
     ]
    }
   ],
   "source": [
    "#fitting on our balanced validation train data seet\n",
    "import time\n",
    "t0 = time.time()\n",
    "clf.fit(Xsm_train,Ysm_train)\n",
    "t1 = time.time()\n",
    "print(\"Time taken for simple XGBoost(for given set of params) is {}\".format(t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction on our validation test data set\n",
    "preds = clf.predict(actual_Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9160022650686896"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(actual_Ytest,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.026785607024297442"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohen_kappa_score(preds,actual_Ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here although good AUC Score, the cohen_kappa_score is very low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8953674647238254"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(preds,actual_Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.030105777054515868"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(preds,actual_Ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With the above parameters got a relatively low performance, this might be due to parameters. Hence we can improve the performance by tuning on hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using SMOTE to balance our entire trian data set\n",
    "sm = SMOTE(ratio='minority', random_state=42)\n",
    "# This will be the data were we are going to \n",
    "Xsm, Ysm = sm.fit_sample(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.01, max_delta_step=0,\n",
       "       max_depth=2, min_child_weight=1, missing=None, n_estimators=700,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=6, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model fitting on our entire train data set balanced\n",
    "clf.fit(Xsm,Ysm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predictions on given test share data.\n",
    "#After oversampling the model is fit on numpy array, we need to convert our test data also to numpy array.\n",
    "test_values = test.values\n",
    "test_pred_share = clf.predict(test_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combining classes with respective ID's into data frame for submission\n",
    "predictions = pd.DataFrame(list(zip(test_df_all['id'],pred_test_share)),columns=[\"id\",\"Target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing to a csv file.\n",
    "predictions.to_csv(\"saikumar_ganneboyina_Finhack.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "With respect to cohen_kappa_score, best model is with Random forests followed by XGBoost. Further after getting the Best Parameters through Randomized search, using Grid Search still better parameters can be achieved.\n",
    "\n",
    "Also the model can be improved by sampling, here though used SMOTE couldn't get better performance. The model needs to be tuned on hyper parameters which can yield a better model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
